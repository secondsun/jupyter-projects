{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Breakdown\n",
    "\n",
    "At last, here in part 3 of 4, it's time to build the models! First, I'll load the analytical base table from Part 2. Then, I'll go through the essential modeling steps:\n",
    "\n",
    "1. [Split the dataset](#split)\n",
    "2. [Build model pipelines](#pipelines)\n",
    "3. [Declare hyperparameters to tune](#hyperparameters)\n",
    "4. [Fit and tune models with cross-validation](#fit-tune)\n",
    "5. [Evaluation metrics](#evaluate)\n",
    "6. [Area under ROC curve](#auroc)\n",
    "\n",
    "Finally, I'll save the best model to use in the next module. I'll spend a little extra time on the last 2 steps because classification tasks require extra metrics compared to regression tasks.\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's import libraries, recruit models, and load the ABT.\n",
    "\n",
    "First, let's import the libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Scikit-Learn for Modeling\n",
    "import sklearn\n",
    "\n",
    "# Pickle for saving model files\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the algorithms I have selected for this project. Given that this is a bivariate classification problem I have decided to go with Logistic Regression, Random Forest, and Boosted Tree algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the Scikit-Learn functions and helpers we'll need for this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn modules for model training\n",
    "# includes modules for splitting training and test set, pipeline creation, standardization, cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Classification metrics for model performance evaluation\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's read the analytical base table I saved at the end of Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analytical base table from Module 2\n",
    "abt_df = pd.read_csv('analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"split\"></span>\n",
    "### 1. Split the dataset\n",
    "\n",
    "I'll start by splitting the data into separate training and test sets. Splitting into test/train sets helps us to minimize the chances of our models overfitting the data by making sure some data is set aside for testing model performance instead of all the data being used to train the model. For this problem I will separate the dataframe into separate objects for the target variable <code style=\"color:steelblue\">y</code>, and the input features, <code style=\"color:steelblue\">X</code>.\n",
    "\n",
    "Once I've created my target variable and feature objects I then use the <code style=\"color:steelblue\">train_test_split()</code> function to split <code style=\"color:steelblue\">X</code> and <code style=\"color:steelblue\">y</code> into training and test sets. I choose to set aside 20% of the observations for the test set, set a <code style=\"color:steelblue\">random_state</code> variabel to ensure replicable results. Most importantly, I pass the argument <code style=\"color:steelblue\">stratify=</code><span style=\"color:crimson\">df.status</span></code> to make sure the target variable's classes are balanced in each subset of data so that we have **stratified random sampling**. This ensures that each of my divided subsets are truly random and the performance I get from my cross-validation is truly representative of my entire training data set. At the end I pring the nubmer of observations in each subset to check that the splitting was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = abt_df.status\n",
    "\n",
    "# Create separate object for input features\n",
    "X = abt_df.drop('status', axis=1)\n",
    "\n",
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234, stratify=abt_df.status)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print('X_train size:', len(X_train))\n",
    "print('X_test size:', len(X_test))\n",
    "print('y_train size:', len(y_train))\n",
    "print('y_test size:', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"pipelines\"></span>\n",
    "### 2. Build model pipelines\n",
    "\n",
    "Next, we'll set up preprocessing pipelines for each of our algorithms. I use the keys below for each algorithm in my dictionary. Each pipeline is standardized first before the algorithm is run. Scaling the data sets so they all have a normal distribution centered around zero prevents any large scaling differences in the data affecting the model performance. Lastly, I set the <code style=\"color:steelblue\">random_state</code> variable for each algorithm to ensure replicable results.\n",
    "\n",
    "* Pipeline dictionary keys:\n",
    "    * <code style=\"color:crimson\">'l1'</code> for $L_1$-regularized logistic regression\n",
    "    * <code style=\"color:crimson\">'l2'</code> for $L_2$-regularized logistic regression\n",
    "    * <code style=\"color:crimson\">'rf'</code> for random forest\n",
    "    * <code style=\"color:crimson\">'gb'</code> for gradient boosted tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipelines = {\n",
    "    'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', random_state=123)),\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', random_state=123)),\n",
    "    'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=123)),\n",
    "    'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"hyperparameters\"></span>\n",
    "### 3. Declare hyperparameters to tune\n",
    "\n",
    "Next, let's declare hyperparameters to tune. I start off by listing the tuneable hyperparameters of the logistic pipeline. There are many listed but usually only one or two are needed for tuning to optimize performance. I declare these hyperparameter grids next for each algorithm. To finish, I create a hyperparameter dictionary which uses the same keys of the pipeline dictionary I created above to store them all. Having both the pipelines and the hyperparameters in dictionary form will make the cross-validation step very straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tuneable hyperparameters of our Logistic pipeline\n",
    "pipelines['l1'].get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "l1_hyperparameters = {\n",
    "    'logisticregression__C': np.linspace(1e-3, 1e3, 10),\n",
    "}\n",
    "l2_hyperparameters = {\n",
    "    'logisticregression__C': np.linspace(1e-3, 1e3, 10),\n",
    "}\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200],\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 0.33],\n",
    "}\n",
    "\n",
    "# Boosted Tree hyperparameters\n",
    "gb_hyperparameters = {\n",
    "    'gradientboostingclassifier__n_estimators': [100, 200],\n",
    "    'gradientboostingclassifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'gradientboostingclassifier__max_depth': [1, 3, 5],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hyperparameters = {\n",
    "    'l1': l1_hyperparameters,\n",
    "    'l2': l2_hyperparameters,\n",
    "    'rf': rf_hyperparameters,\n",
    "    'gb': gb_hyperparameters,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"fit-tune\"></span>\n",
    "### 4. Fit and tune models with cross-validation\n",
    "\n",
    "Now that we have our <code style=\"color:steelblue\">pipelines</code> and <code style=\"color:steelblue\">hyperparameters</code> dictionaries declared, we're ready to tune our models with **cross-validation**.\n",
    "\n",
    "For this task I start by creating an empty <code style=\"color:SteelBlue\">fitted_models</code> dictionary that will include models that have been tuned using cross-validation. The keys will be the same as those in the <code style=\"color:SteelBlue\">pipelines</code> and <code style=\"color:SteelBlue\">hyperparameters</code> dictionaries. The values will be <code style=\"color:steelblue\">GridSearchCV</code> objects that have been fitted to <code style=\"color:steelblue\">X_train</code> and <code style=\"color:steelblue\">y_train</code>. After fitting each model, I will print <code style=\"color:crimson\">'{name} has been fitted.'</code> just to track the progress. To speed up processing time I set <code style=\"color:steelblue\">n_jobs=<span style=\"color:crimson\">-1</span></code> to use as many cores as available on my computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline, hyperparameters[name], cv=10, n_jobs=-1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"evaluate\"></span>\n",
    "### 5. Evaluation metrics\n",
    "\n",
    "Finally, it's time to evaluate our models and pick the best one. Here I create a for loop which calculates the <code style=\"color:steelblue\">best\\_score_</code> attribute for each fitted model. As you can see, the ensemble tree algorithms had the best performance with Random Forest model getting the highest score followed by Boosted Trees and then the two Logistic Regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best_score_ for each fitted model\n",
    "for name, model in fitted_models.items():\n",
    "    print(name, model.best_score_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"auroc\"></span>\n",
    "### 6. Area under ROC curve\n",
    "\n",
    "**Area under ROC curve** is the most reliable metric for classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before presenting the idea of an ROC curve, we must first discuss what a **confusion matrix** is. This is essentially a chart that gives probabilities of false positives, true positives, false negatives, and true negatives so that we have deeper visibility into the performance of our models. \n",
    "\n",
    "Let's see an example using our $L_1$-regularized logistic regression. First, I'll use <code style=\"color:steelblue\">.predict()</code> to get the predicted classes directly. Then I'll print out the confusion matrix directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using L1-regularized logistic regression \n",
    "pred = fitted_models['l1'].predict(X_test)\n",
    "\n",
    "# Display first 5 predictions\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict a **probability** for each class using <code style=\"color:steelblue\">.predict_proba()</code>, instead of the class directly.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using L1-regularized logistic regression\n",
    "pred = fitted_models['l1'].predict_proba(X_test)\n",
    "\n",
    "# Get just the prediction for the positive class (1)\n",
    "pred = [p[1] for p in pred]\n",
    "\n",
    "# Display first 5 predictions\n",
    "pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the ROC curve using the <code style=\"color:steelblue\">roc_curve()</code> function that we imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve from y_test and pred\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can throw these into a DataFrame for convenience and look at the last 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store fpr, tpr, thresholds in DataFrame and display last 10\n",
    "pd.DataFrame({'FPR': fpr, 'TPR': tpr, 'Thresholds': thresholds}).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as you decrease the threshold, both the false positive rate **and** the true positive rate increase.\n",
    "\n",
    "We can plot the entire curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='l1')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Diagonal 45 degree line\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "\n",
    "# Axes limits and labels\n",
    "plt.xlim([-0.1, 1.1])\n",
    "plt.ylim([-0.1, 1.1])\n",
    "plt.xlabel('True Positive Rate')\n",
    "plt.ylabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate AUROC, use the <code style=\"color:steelblue\">auc()</code> function we imported earlier in conjunction with the <code style=\"color:steelblue\">roc_curve()</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "# Calculate AUROC\n",
    "print(auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a detour to dive into some of the intuition behind AUROC, let's calculate it for each of our fitted models on the test set. I'm going to print the <code style=\"color:SteelBlue\">auc</code> of the <code style=\"color:SteelBlue\">roc_curve</code> and label the output with the name of the algorithm. Finally, I will save the winning <code style=\"color:steelblue\">Pipeline</code> object into a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "for name, model in fitted_models.items():\n",
    "    pred = model.predict_proba(X_test)\n",
    "    pred = [p[1] for p in pred]\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred)\n",
    "    print(name, auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save winning model as final_model.pkl\n",
    "with open ('final_model.pkl', 'wb') as f:\n",
    "    pickle.dump(fitted_models['rf'].best_estimator_, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "As a reminder, here are a few things we did in this module:\n",
    "* Split the dataset into training and test sets.\n",
    "* Set up model pipelines and hyperparameter grids.\n",
    "* Tuned the models using cross-validation.\n",
    "* Learned about how AUROC is a more effective metric for classification than simple accuracy.\n",
    "* Saved the winning model.\n",
    "\n",
    "In the next module, <span style=\"color:royalblue\">Part 4: Project Delivery</span>, we'll see how we can go the extra mile in terms of project delivery. \n",
    "\n",
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
